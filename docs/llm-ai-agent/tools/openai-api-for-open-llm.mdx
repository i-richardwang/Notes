---
title: "开源大模型兼容Openai接口"
description: "介绍如何通过 api-for-open-llm 项目部署并调用开源大模型，实现与 OpenAI API 兼容的后端接口。"
---


本文介绍一下如何使用 [api-for-open-llm](https://github.com/xusenlinzy/api-for-open-llm) 项目来部署和调用开源大型语言模型。此项目提供了一种方法，使得开源模型能以与 OpenAI ChatGPT API 类似的方式被调用，并且支持流式响应、文本嵌入模型、langchain 开发工具等多种功能。

## 特性概览

`api-for-open-llm` 项目具备以下特性（复制自官网）：

> - ✨ 以 OpenAI ChatGPT API 的方式调用各类开源大模型
> - 🖨️ 支持流式响应，实现打印机效果
> - 📖 实现文本嵌入模型，为文档知识问答提供支持
> - 🦜️ 支持大规模语言模型开发工具 langchain 的各类功能
> - 🙌 只需要简单的修改环境变量即可将开源模型作为 chatgpt 的替代模型，为各类应用提供后端支持
> - 🚀 支持加载经过自行训练过的 lora 模型
> - ⚡ 支持 vLLM 推理加速和处理并发请求

## 使用步骤

import { Steps } from '@theme';

<Steps>

### 克隆仓库并进入目录

首先，您需要从 GitHub 克隆 `api-for-open-llm` 仓库到本地，并进入项目目录：

```bash
git clone https://github.com/xusenlinzy/api-for-open-llm.git
cd api-for-open-llm
```

### 安装依赖

```bash
pip install -r requirements.txt
```

### 配置环境变量

复制 `.env.example` 文件为 `.env`，然后根据需要修改环境变量：

```bash
cp .env.example .env
```

### 启动服务

将 `server.py` 文件复制到当前目录，并使用 Python 启动服务：

```bash
cp api/server.py .
python server.py
```

</Steps>

## 环境变量配置示例

1. **Qwen 模型配置**

    ```yml
    MODEL_NAME=qwen
    MODEL_PATH=Qwen/Qwen-14B-Chat-Int4
    DEVICE_MAP=auto
    ```

2. **Yi-Chat 模型配置**

    ```yaml
    MODEL_NAME=yi-chat
    MODEL_PATH=01-ai/Yi-34B-Chat
    PROMPT_NAME=yi
    DEVICE_MAP=auto
    ```

## 模型调用

```python
openai_api_key="xxx",                     # 可以任意填写
openai_api_base="http://ip:8000/v1",      # .env 文件中配置的端口
```